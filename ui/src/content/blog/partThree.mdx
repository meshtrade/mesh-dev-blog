---
title: AI Visualised - Part III
description: Looking at how we can build an intuition for neural networks visually
pubDate: 2025-03-16T12:04:48.328Z
heroImage: /partThree.jpg
author: Kyle Smith
audioURL: /audio/partTwoHalf.wav
---

import { Image } from 'astro:assets';
import Plot from "../../components/Plot.astro";
import Scroll from "../../components/Scroll.astro";
import EmbeddingVis from "../../components/EmbeddingVis.astro";
import Done from "../../components/Done.astro";

<Scroll id="AI Visualised - Part III" />

We've covered a lot of ground, and now we arrive at Deep Learning. Our exploration began with the perceptron, a fundamental unit adept at classifying linearly separable data. By interconnecting perceptrons, we created neural networks, significantly boosting their capacity to process and interpret complex data. We then explored embeddings, techniques that translate unstructured data into structured representations, revealing deeper insights into how neural networks encode and understand information. As an introduction to Deep Learning, we'll focus on Large Language Models (LLMs), specifically the classic transformer architecture. This approach diverges from the traditional use of Convolutional Neural Networks (CNNs), often used to introduce Deep Learning concepts.
